{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ba84b0",
   "metadata": {},
   "source": [
    "Script to test PDGrapher on chemical/genetic perturbation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "import os\n",
    "import os.path as osp\n",
    "torch.set_num_threads(20)\n",
    "from datetime import datetime\n",
    "from pdgrapher import PDGrapher\n",
    "from pdgrapher import Trainer, Dataset\n",
    "import sys\n",
    "from glob import glob\n",
    "from pdgrapher._utils import get_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11b69e",
   "metadata": {},
   "source": [
    "Test on chemical or genetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"Chemical\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4dca7d",
   "metadata": {},
   "source": [
    "Define the corresponding cell lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a58524",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type == \"chemical\":\n",
    "    cell_lines = ['A549', 'MCF7', 'PC3', 'VCAP', 'MDAMB231', 'BT20', 'HT29', 'A375', 'HELA']\n",
    "elif data_type == \"genetic\":\n",
    "    cell_lines = ['PC3', 'YAPC', 'AGS', 'A375', 'HT29', 'A549', 'BICR6', 'U251MG', 'ES2', 'MCF7']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f7d20",
   "metadata": {},
   "source": [
    "Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaff3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_backward_data = True\n",
    "use_supervision = True #whether to use supervision loss\n",
    "use_intervention_data = True #whether to use cycle loss\n",
    "current_date = datetime.now() \n",
    "n_layers_nn = 1\n",
    "global use_forward_data\n",
    "use_forward_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b4b4d",
   "metadata": {},
   "source": [
    "Load model and do test and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb367236",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell_line in cell_lines:\n",
    "    print(f\"Processing cell line: {cell_line}\")\n",
    "\n",
    "    if data_type == 'chemical':\n",
    "        if cell_line in ['HA1E', 'HT29', 'A375', 'HELA']:\n",
    "            use_forward_data = False\n",
    "\n",
    "        #Dataset\n",
    "        dataset = Dataset(\n",
    "            forward_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/data_forward_{cell_line}.pt\",\n",
    "            backward_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/data_backward_{cell_line}.pt\",\n",
    "            splits_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/PDgrapher/data/split_guada/{data_type}/{data_type}/{cell_line}/random/5fold/splits.pt\"\n",
    "        )\n",
    "\n",
    "        edge_index = torch.load(f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/edge_index_{cell_line}.pt\")\n",
    "\n",
    "        #Modify based on folder name\n",
    "        paths = glob('/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/PDgrapher/output/{}/{}_corrected_pos_emb/*'.format(data_type, cell_line))\n",
    "    \n",
    "    elif data_type == 'genetic':\n",
    "        if cell_line in ['ES2', 'BICR6', 'YAPC', 'AGS', 'U251MG', 'HT29', 'A375']:\n",
    "            use_forward_data = False\n",
    "\n",
    "        #Dataset\n",
    "        dataset = Dataset(\n",
    "            forward_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/data_forward_{cell_line}.pt\",\n",
    "            backward_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/data_backward_{cell_line}.pt\",\n",
    "            splits_path=f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/PDgrapher/data/split_guada/{data_type}/{data_type}/{cell_line}/random/5fold/splits.pt\"\n",
    "        )\n",
    "\n",
    "        edge_index = torch.load(f\"/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/lincs_processed/{data_type}/pt_data/real_lognorm/edge_index_{cell_line}.pt\")\n",
    "\n",
    "        #Modify based on folder name\n",
    "        paths = glob('/n/holystore01/LABS/mzitnik_lab/Lab/xianglin226/PDgrapher/output/{}/{}_corrected_pos_emb/*'.format(data_type, cell_line))\n",
    "\n",
    "    for path in paths:\n",
    "\n",
    "        outdir = './results_pdgrapher/{}/val/'.format(data_type)\n",
    "        path_model = path\n",
    "\n",
    "        n_layers_gnn = int(path.split('/')[-1].split('_')[2])\n",
    "\n",
    "        all_recall_at_1 = {'test':[], 'val':[]}\n",
    "        all_recall_at_10 = {'test':[], 'val':[]}\n",
    "        all_recall_at_100 = {'test':[], 'val':[]}\n",
    "        all_recall_at_1000 = {'test':[], 'val':[]}\n",
    "        all_perc_partially_accurate_predictions = {'test':[], 'val':[]}\n",
    "        all_rankings = {'test':[], 'val':[]}\n",
    "\n",
    "        for fold in range(1,6):\n",
    "            #Instantiates model\n",
    "            model = PDGrapher(edge_index, model_kwargs={\"n_layers_nn\": 1, \"n_layers_gnn\": n_layers_gnn, \"num_vars\": dataset.get_num_vars(),\n",
    "                                                        },\n",
    "                                        response_kwargs={'train': True},\n",
    "                                        perturbation_kwargs={'train':True})\n",
    "\n",
    "            # restore response prediction\n",
    "            save_path = osp.join(path, '_fold_{}_response_prediction.pt'.format(fold))\n",
    "            checkpoint = torch.load(save_path)\n",
    "            model.response_prediction.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            # restore Perturbation discovery\n",
    "            save_path = osp.join(path, '_fold_{}_perturbation_discovery.pt'.format(fold))\n",
    "            checkpoint = torch.load(save_path)\n",
    "            model.perturbation_discovery.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "            #loads fold-specific dataset\n",
    "            device = torch.device('cuda')\n",
    "\n",
    "            dataset.prepare_fold(fold)\n",
    "\n",
    "            thresholds = get_thresholds(dataset)\n",
    "            thresholds = {k: v.to(device) if v is not None else v for k, v in thresholds.items()} \n",
    "\n",
    "            model.response_prediction.edge_index = model.response_prediction.edge_index.to(device)\n",
    "            model.perturbation_discovery.edge_index = model.perturbation_discovery.edge_index.to(device)\n",
    "            model.perturbation_discovery = model.perturbation_discovery.to(device)\n",
    "\n",
    "            model.perturbation_discovery.eval()\n",
    "            model.response_prediction.eval()\n",
    "\n",
    "            (\n",
    "                        train_loader_forward, train_loader_backward,\n",
    "                        val_loader_forward, val_loader_backward,\n",
    "                        test_loader_forward, test_loader_backward\n",
    "                    ) = dataset.get_dataloaders(num_workers = 20, batch_size = 1)\n",
    "\n",
    "            recall_at_1 = []\n",
    "            recall_at_10 = []\n",
    "            recall_at_100 = []\n",
    "            recall_at_1000 = []\n",
    "            perc_partially_accurate_predictions = []\n",
    "            rankings = []\n",
    "            n_non_zeros = 0\n",
    "\n",
    "            ####ON TEST SET\n",
    "            for data in test_loader_backward:\n",
    "                pred_backward_m2 = model.perturbation_discovery(torch.concat([data.diseased.view(-1, 1).to(device), data.treated.view(-1, 1).to(device)], 1), data.batch.to(device), mutilate_mutations=data.mutations.to(device), threshold_input=thresholds)\n",
    "                out = pred_backward_m2\n",
    "                                \n",
    "                num_nodes = int(data.num_nodes / len(torch.unique(data.batch)))            \n",
    "                \n",
    "                correct_interventions = set(torch.where(data.intervention.detach().cpu().view(-1, num_nodes))[1].tolist())\n",
    "                predicted_interventions = torch.argsort(out.detach().cpu().view(-1, num_nodes), descending=True)[0, :].tolist()\n",
    "\n",
    "                for ci in list(correct_interventions):\n",
    "                    rankings.append(1 - (predicted_interventions.index(ci) / num_nodes))\n",
    "                \n",
    "                recall_at_1.append(len(set(predicted_interventions[:1]).intersection(correct_interventions)) / len(correct_interventions))\n",
    "                recall_at_10.append(len(set(predicted_interventions[:10]).intersection(correct_interventions)) / len(correct_interventions))\n",
    "                recall_at_100.append(len(set(predicted_interventions[:100]).intersection(correct_interventions)) / len(correct_interventions))\n",
    "                recall_at_1000.append(len(set(predicted_interventions[:1000]).intersection(correct_interventions)) / len(correct_interventions))\n",
    "\n",
    "                jaccards = len(correct_interventions.intersection(predicted_interventions[:len(correct_interventions)])) / len(correct_interventions.union(predicted_interventions))\n",
    "\n",
    "                if jaccards != 0:\n",
    "                    n_non_zeros += 1\n",
    "\n",
    "            all_recall_at_1['test'].append(np.mean(recall_at_1))\n",
    "            all_recall_at_10['test'].append(np.mean(recall_at_10))\n",
    "            all_recall_at_100['test'].append(np.mean(recall_at_100))\n",
    "            all_recall_at_1000['test'].append(np.mean(recall_at_1000))\n",
    "            all_rankings['test'].append(np.mean(rankings))\n",
    "            all_perc_partially_accurate_predictions['test'].append(100 * n_non_zeros/len(test_loader_backward))\n",
    "            print('fold {}/5'.format(fold))\n",
    "\n",
    "            ####ON VALIDATION SET\n",
    "            recall_at_1 = []\n",
    "            recall_at_10 = []\n",
    "            recall_at_100 = []\n",
    "            recall_at_1000 = []\n",
    "            perc_partially_accurate_predictions = []\n",
    "            rankings = []\n",
    "            n_non_zeros = 0\n",
    "            \n",
    "            \n",
    "            for data in val_loader_backward:\n",
    "                pred_backward_m2 = model.perturbation_discovery(torch.concat([data.diseased.view(-1, 1).to(device), data.treated.view(-1, 1).to(device)], 1), data.batch.to(device), mutilate_mutations=data.mutations.to(device), threshold_input=thresholds)\n",
    "                out = pred_backward_m2\n",
    "                                \n",
    "                num_nodes = int(data.num_nodes / len(torch.unique(data.batch)))\n",
    "                \n",
    "                \n",
    "                correct_interventions = set(torch.where(data.intervention.detach().cpu().view(-1, num_nodes))[1].tolist())\n",
    "                predicted_interventions = torch.argsort(out.detach().cpu().view(-1, num_nodes), descending=True)[0, :].tolist()\n",
    "\n",
    "                for ci in list(correct_interventions):\n",
    "                    rankings.append(1 - (predicted_interventions.index(ci) / num_nodes))\n",
    "                \n",
    "                recall_at_1.append(len(set(predicted_interventions[:1]).intersection(correct_interventions)) / len(correct_interventions))\n",
    "                recall_at_10.append(len(set(predicted_interventions[:10]).intersection(correct_interventions)) / len(correct_interventions))\n",
    "                recall_at_100.append(len(set(predicted_interventions[:100]).intersection(correct_interventions)) / len(correct_interventions))\n",
    "                recall_at_1000.append(len(set(predicted_interventions[:1000]).intersection(correct_interventions)) / len(correct_interventions))\n",
    "\n",
    "\n",
    "                jaccards = len(correct_interventions.intersection(predicted_interventions[:len(correct_interventions)])) / len(correct_interventions.union(predicted_interventions))\n",
    "\n",
    "                if jaccards != 0:\n",
    "                    n_non_zeros += 1\n",
    "\n",
    "\n",
    "            all_recall_at_1['val'].append(np.mean(recall_at_1))\n",
    "            all_recall_at_10['val'].append(np.mean(recall_at_10))\n",
    "            all_recall_at_100['val'].append(np.mean(recall_at_100))\n",
    "            all_recall_at_1000['val'].append(np.mean(recall_at_1000))\n",
    "            all_rankings['val'].append(np.mean(rankings))\n",
    "            all_perc_partially_accurate_predictions['val'].append(100 * n_non_zeros/len(test_loader_backward))\n",
    "            print('fold {}/5'.format(fold))\n",
    "\n",
    "\n",
    "        log = open(osp.join(outdir, f'{cell_line}_{n_layers_gnn}_final_performance_metrics_within.txt'), 'w')\n",
    "        log.write('\\n\\nVALIDATION SET\\n')\n",
    "        log.write('recall@1: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_1['val']), np.std(all_recall_at_1['val'])))\n",
    "        log.write('recall@10: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_10['val']), np.std(all_recall_at_10['val'])))\n",
    "        log.write('recall@100: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_100['val']), np.std(all_recall_at_100['val'])))\n",
    "        log.write('recall@1000: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_1000['val']), np.std(all_recall_at_1000['val'])))\n",
    "        log.write('percentage of samples with partially accurate predictions: {:.2f}±{:.2f}\\n'.format(np.mean(all_perc_partially_accurate_predictions['val']), np.std(all_perc_partially_accurate_predictions['val'])))\n",
    "        log.write('ranking score: {:.2f}±{:.2f}\\n'.format(np.mean(all_rankings['val']), np.std(all_rankings['val'])))\n",
    "\n",
    "        log.write('--------------------------\\n')\n",
    "        log.write('All metric datapoints:\\n')\n",
    "        log.write('recall@1: {}\\n'.format(all_recall_at_1['val']))\n",
    "        log.write('recall@10: {}\\n'.format(all_recall_at_10['val']))\n",
    "        log.write('recall@100: {}\\n'.format(all_recall_at_100['val']))\n",
    "        log.write('recall@1000: {}\\n'.format(all_recall_at_1000['val']))\n",
    "        log.write('percentage of samples with partially accurate predictions: {}\\n'.format(all_perc_partially_accurate_predictions['val']))\n",
    "        log.write('ranking score: {}\\n'.format(all_rankings['val']))\n",
    "\n",
    "        log.write('\\n\\n----------------------\\n')\n",
    "        log.write('\\n\\nTEST SET\\n')\n",
    "        log.write('recall@1: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_1['test']), np.std(all_recall_at_1['test'])))\n",
    "        log.write('recall@10: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_10['test']), np.std(all_recall_at_10['test'])))\n",
    "        log.write('recall@100: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_100['test']), np.std(all_recall_at_100['test'])))\n",
    "        log.write('recall@1000: {:.4f}±{:.4f}\\n'.format(np.mean(all_recall_at_1000['test']), np.std(all_recall_at_1000['test'])))\n",
    "        log.write('percentage of samples with partially accurate predictions: {:.2f}±{:.2f}\\n'.format(np.mean(all_perc_partially_accurate_predictions['test']), np.std(all_perc_partially_accurate_predictions['test'])))\n",
    "        log.write('ranking score: {:.2f}±{:.2f}\\n'.format(np.mean(all_rankings['test']), np.std(all_rankings['test'])))\n",
    "\n",
    "        log.write('--------------------------\\n')\n",
    "        log.write('All metric datapoints:\\n')\n",
    "        log.write('recall@1: {}\\n'.format(all_recall_at_1['test']))\n",
    "        log.write('recall@10: {}\\n'.format(all_recall_at_10['test']))\n",
    "        log.write('recall@100: {}\\n'.format(all_recall_at_100['test']))\n",
    "        log.write('recall@1000: {}\\n'.format(all_recall_at_1000['test']))\n",
    "        log.write('percentage of samples with partially accurate predictions: {}\\n'.format(all_perc_partially_accurate_predictions['test']))\n",
    "        log.write('ranking score: {}\\n'.format(all_rankings['test']))\n",
    "\n",
    "        log.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
